# -*- coding: utf-8 -*-
"""LeNet5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1idAlvWDHdkqyJTwqgWthmMv15KCVt78Q
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torchsummary import summary
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt

BATCH_SIZE = 64
transforms_ = transforms.Compose([transforms.Resize((32, 32)),
                                 transforms.ToTensor()])

# download and create datasets
train_dataset = datasets.MNIST(root='/content/drive/MyDrive/828C/proj2/part1/data', 
                               train=True, 
                               transform=transforms_,
                               download=True)
test_dataset = datasets.MNIST(root='/content/drive/MyDrive/828C/proj2/part1/data', 
                               train=False, 
                               transform=transforms_)

# define the data loaders
train_loader = DataLoader(dataset=train_dataset, 
                          batch_size=BATCH_SIZE, 
                          shuffle=True, drop_last=True)

test_loader = DataLoader(dataset=test_dataset, 
                          batch_size=BATCH_SIZE, 
                          shuffle=False, drop_last=True)

class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        
        self.features = nn.Sequential(            
            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size=2),
            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size=2),
            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),
            nn.ReLU()
        )
        self.classifier = nn.Sequential(
            nn.Linear(in_features=120, out_features=84),
            nn.ReLU(),
            nn.Linear(in_features=84, out_features=10),
        )

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        logits = self.classifier(x)
        probs = F.softmax(logits, dim=1)
        
        return probs

model = LeNet5()

# parameters
LEARNING_RATE = 0.001
EPOCHS = 15
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.001)
criterion = nn.CrossEntropyLoss()

def train():
    
    train_loss = []
    test_loss = []
    train_accuracy_list = []
    test_accuracy_list = []

    for epoch in range(EPOCHS):
        running_loss = 0.0
        running_test_loss = 0.0
        correct_predictions = 0.0

        model.train()
        for (data, target) in train_loader:
            optimizer.zero_grad()
            output = model(data.view(BATCH_SIZE, 1, 32, 32))
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            running_loss += loss.detach().item()

            index = output.max(dim=1)[1]
            correct_predictions += (index == target).sum().detach().item()

        train_accuracy = 100*(correct_predictions / (len(train_loader)*BATCH_SIZE))
        train_accuracy_list.append(train_accuracy)

        correct_predictions = 0.0

        with torch.no_grad():
            model.eval()
            for (data, target) in test_loader:
                output = model(data.view(BATCH_SIZE, 1, 32, 32))
                loss = criterion(output, target)
                running_test_loss += loss.detach().item()

                index = output.max(dim=1)[1]
                correct_predictions += (index == target).sum().detach().item()

        avg_train_loss = running_loss / len(train_loader)
        train_loss.append(avg_train_loss)

        avg_test_loss = running_test_loss / len(test_loader)
        test_loss.append(avg_test_loss)

        test_accuracy = 100 * (correct_predictions / (len(test_loader) * BATCH_SIZE))
        test_accuracy_list.append(test_accuracy)
        
        print('Epoch {}, Train Loss: {:.4f}, Test Loss: {:.4f}, Train Accuracy: {:.3f}, Test Accuracy: {:.3f}'.format(epoch+1, avg_train_loss, avg_test_loss, train_accuracy, test_accuracy))

    return train_loss, test_loss, train_accuracy_list, test_accuracy_list

train_loss, test_loss, train_accuracy_list, test_accuracy_list = train()

plt.plot(list(range(1, EPOCHS + 1)), train_loss, color='b', label='Training loss')
plt.plot(list(range(1, EPOCHS + 1)), test_loss, color='r', label='Test loss')
plt.xlabel('Number of epochs')
plt.ylabel('Cross Entropy Loss')
plt.title('LeNet5 on MNIST dataset - Loss vs No. of epochs')
plt.legend()
plt.savefig('/content/drive/MyDrive/828C/proj2/part1/plots/LeNet5_loss.png')
plt.show()
